<!DOCTYPE HTML>
<html lang="en">

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Kevin Lin</title>

    <meta name="author" content="Kevin Lin">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
            <tr style="padding:0px">
                <td style="padding:0px">
                    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr style="padding:0px">
                                <td style="padding:2.5%;width:63%;vertical-align:middle">
                                    <p style="text-align:center">
                                        <name>Kevin Lin</name>
                                    </p>
                                    <p>PhD Computer Science at UT Austin, advised by <a href="https://yukezhu.me/">Yuke Zhu</a>; research intern at <a href="https://research.nvidia.com/labs/gear/">Nvidia GEAR</a> (Generalist Embodied Agent Research) Lab.
                                        Previously, MS Computer Science at Stanford, advised by <a href="https://web.stanford.edu/~bohg/">Jeannette Bohg</a>; 
                                        EECS at UC Berkeley. I did research at <a href="https://waabi.ai/">Waabi</a>, advised by Raquel Urtasun, and at 
                                        <a href="https://bair.berkeley.edu/">Berkeley Artificial Intelligence Research </a> with members of <a href="https://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel's</a> Robot Learning Lab. 
                                        I also did research with <a href="https://smlbansal.github.io/">Somil Bansal</a>.
                                    </p>

                                    <p>If you're interested in collaborating or getting into robotics, feel free to reach out!</p>

                                    <p style="text-align:center">
                                        <a href="mailto:kevinlin@cs.utexas.edu">Email</a> &nbsp/&nbsp
                                        <a href="https://www.linkedin.com/in/kevin-thankyou-lin/">LinkedIn</a> &nbsp/&nbsp
                                        <a href="https://github.com/kevin-thankyou-lin">Github</a> &nbsp/&nbsp
                                        <a href="https://scholar.google.com/citations?hl=en&user=CTxv7koAAAAJ">Google Scholar</a>
                                    </p>
                                </td>
                                <td style="padding:2.5%;width:40%;max-width:40%">
                                    <a><img style="width:100%;max-width:100%;border-radius:50%;" alt="profile photo" src="./images/kevinlin.png" class="hoverZoomLink rounded-circle"></a>
                                </td>
                            </tr>
                        </tbody>
                    </table>
                    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <heading>Research</heading>
                                    <p>
                                        I'm interested in developing general purpose robots. 
                                    </p>
                                    <p>
                                        Previously, I have leveraged foundation models for robust and verified 
                                        long horizon planning for robot manipulation from natural language instructions.

                                        Currently, I'm developing a general purpose robotic manipulation framework to enable high reliability on tasks when provided a few demonstrations.
                                    </p>
                                    <p>
                                        In the future, I aim to develop a general purpose robotics foundation model that can be finetuned for specific robot embodiments and settings.
                                        I plan to achieve this goal by leveraging a diverse range of data sources --- real world robot data, simulation data, human videos --- and using
                                        methods from machine learning, vision, graphics, and robotics. 
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table>
                    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>

                            <tr onmouseout="groot_stop()" onmouseover="groot_start()" class="spaceUnder">
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <div class="one">
                                        <div class="two" id="groot_image">
                                            <img src="images/gr00t_n1.jpg" width="160"></div>
                                        <img src="images/gr00t_n1.jpg" width="160">
                                    </div>
                                    <script type="text/javascript">
                                        function groot_start() {
                                          document.getElementById('groot_image').style.opacity = "1";
                                        }
                        
                                        function groot_stop() {
                                          document.getElementById('groot_image').style.opacity = "0";
                                        }
                                        groot_stop()
                                    </script>
                                </td>
                                <td style="padding:20px;width:75%;vertical-align:middle">
                                    <a href="https://developer.nvidia.com/isaac/gr00t"><papertitle>GR00T N1/N1.5/N1.6: Open Foundation Models for Generalist Humanoid Robots</papertitle></a> 
                                    <br>
                                    NVIDIA GR00T Team
                                    <br><br>
                                    <a href="https://arxiv.org/abs/2503.14734"><em>Technical Report</em>, 2025</a>
                                    <br>
                                    <a href="https://developer.nvidia.com/blog/"><em>NVIDIA Blog</em></a> /
                                    <a href="https://techcrunch.com/"><em>TechCrunch</em></a> /
                                    <a href="https://www.theverge.com/"><em>The Verge</em></a>
                                    <p>
                                        We introduce GR00T N1, N1.5, and N1.6, open foundation models for humanoid robots. 
                                        These Vision-Language-Action (VLA) models feature an end-to-end dual-system architecture: the vision-language module (System 2) interprets the environment, while the diffusion transformer (System 1) generates real-time motor actions.
                                        GR00T models outperform state-of-the-art imitation learning baselines on standard simulation benchmarks across multiple robot embodiments.
                                    </p>
                                </td>
                            </tr>

                            <tr onmouseout="cpgen_stop()" onmouseover="cpgen_start()">
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <div class="one" style="margin-top: 0px;">
                                        <div class="two" id="cpgen_image">
                                          <video src="videos/cpgen.mp4" width="160" autoplay loop muted></video>
                                        </div>
                                        <video src="videos/cpgen.mp4" width="160" autoplay loop muted></video>
                                    </div>
                                    <script type="text/javascript">
                                        function cpgen_start() {
                                          document.getElementById('cpgen_image').style.opacity = "1";
                                        }
                        
                                        function cpgen_stop() {
                                          document.getElementById('cpgen_image').style.opacity = "0";
                                        }
                                        cpgen_stop()
                                    </script>
                                </td>
                                <td style="padding:20px;width:75%;vertical-align:middle">
                                    <a href="https://cp-gen.github.io/">
                                        <papertitle>Constraint-Preserving Data Generation for Visuomotor Policy Generalization</papertitle>
                                    </a>
                                        <br>
                                        <strong>Kevin Lin</strong>,
                                        <a href="https://openreview.net/profile?id=%7EVarun_Ragunath1">Varun Ragunath*</a>,
                                        <a href="https://www.linkedin.com/in/andrewmcalinden">Andrew McAlinden*</a>,
                                        <a href="https://www.linkedin.com/in/aaditya-prasad/">Aaditya Prasad</a>,
                                        <a href="https://www.cs.princeton.edu/~jw60/">Jimmy Wu</a>,
                                        <a href="https://www.cs.utexas.edu/~yukez/">Yuke Zhu</a>,
                                        <a href="https://web.stanford.edu/~bohg/">Jeannette Bohg</a>
                                        <br>
                                        <br>
                                        <a href="https://www.corl.org/"><em>Conference on Robot Learning</em>, 2025</a> &nbsp
                                    <p>
                                        We present CP-Gen, a synthetic data generation framework that uses a single expert trajectory to generate 1000s of robot demonstrations containing novel object geometries and poses. 
                                        These generated demonstrations are used to train closed-loop visuomotor policies that transfer zero-shot from simulation to the real world.
                                </td>
                            </tr>

                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <div class="one">
                                        <div class="two" id='dualrefl_image'>
                                            <img src='images/robosuite.jpg' width="160"></div>
                                        <img src='images/robosuite.jpg' width="160">
                                    </div>
                                    <script type="text/javascript">
                                        function dualrefl_start() {
                                          document.getElementById('dualrefl_image').style.opacity = "1";
                                        }
                        
                                        function dualrefl_stop() {
                                          document.getElementById('dualrefl_image').style.opacity = "0";
                                        }
                                        dualrefl_stop()
                                    </script>
                                </td>
                                <td style="padding:20px;width:75%;vertical-align:middle">
                                    <a href="https://robosuite.ai/"><papertitle>robosuite: A modular simulation framework and benchmark for robot learning</papertitle></a>
                                    <br>
                                    <a href="https://www.cs.utexas.edu/~yukez/">Yuke Zhu</a>,
                                    <a href="https://www.jowo.me/about">Josiah Wong</a>,
                                    <a href="https://ai.stanford.edu/~amandlek/">Ajay Mandlekar</a>,
                                    <a href="https://robertomartinmartin.com/">Roberto Martín-Martín</a>,
                                    Abhishek Joshi,
                                    <strong>Kevin Lin</strong>,
                                    Abhiram Maddukuri,
                                    <a href="https://snasiriany.me/">Soroush Nasiriany</a>,
                                    <a href="https://zhuyifengzju.github.io/">Yifeng Zhu</a>
                                    <br><br>
                                    <a href="https://arxiv.org/abs/2009.12293"><em>Technical Report</em>, 2025</a> &nbsp
                                    <p>
                                        robosuite is a modular simulation framework and benchmark for robot learning, providing a rich set of environments for manipulation research with various robot arms and grippers.
                                    </p>
                                </td>
                            </tr>

                            <tr onmouseout="dualrefl_stop()" onmouseover="dualrefl_start()">
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <div class="one" style="margin-top: 0px;">
                                        <div class="two" id="dualrefl_image">
                                          <video src="videos/dexmg.mp4" width="160" autoplay loop muted></video>
                                        </div>
                                        <video src="videos/dexmg.mp4" width="160" autoplay loop muted></video>
                                    </div>
                                    <script type="text/javascript">
                                        function dualrefl_start() {
                                          document.getElementById('dualrefl_image').style.opacity = "1";
                                        }
                        
                                        function dualrefl_stop() {
                                          document.getElementById('dualrefl_image').style.opacity = "0";
                                        }
                                        dualrefl_stop()
                                    </script>
                                </td>
                                <td style="padding:20px;width:75%;vertical-align:middle">
                                    <a href="https://dexmimicgen.github.io/">
                                        <papertitle>DexMimicGen: Automated Data Generation for Bimanual Dexterous Manipulation via Imitation Learning</papertitle>
                                    </a>
                                        <br>
                                        <strong>Kevin Lin*</strong>,
                                        <a href="https://zhenyujiang.me/">Zhenyu Jiang*</a>,
                                        <a href="https://xieleo5.github.io/">Yuqi Xie*</a>,
                                        <a href="https://www.zhenjiaxu.com/">Zhenjia Xu</a>,
                                        <a href="https://wkwan7.github.io/">Weikang Wan</a>,
                                        <a href="https://ai.stanford.edu/~amandlek/">Ajay Mandlekar†</a>,
                                        <a href="https://jimfan.me/">Jim Fan†</a>,
                                        <a href="https://www.cs.utexas.edu/~yukez/">Yuke Zhu†</a>
                                        <br>
                                        <br>
                                        <a href="https://2025.ieee-icra.org/"><em>International Conference on Robotics and Automation</em>, 2025</a> &nbsp
                                    <p>
                                        We introduce DexMimicGen, a large-scale automated data generation system that synthesizes trajectories from a handful of human demonstrations for humanoid robots with dexterous hands.
                                        We leverage DexMimicGen to generate data and deploy a trained policy on a real-world humanoid can sorting task.
                                </td>
                            </tr>

                            <tr onmouseout="dualrefl_stop()" onmouseover="dualrefl_start()">
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <div class="one" style="margin-top: 0px;">
                                        <div class="two" id="dualrefl_image">
                                          <video src="videos/cp-dp.mp4" width="160" autoplay loop muted></video>
                                        </div>
                                        <video src="videos/cp-dp.mp4" width="160" autoplay loop muted></video>
                                    </div>
                                    <script type="text/javascript">
                                        function dualrefl_start() {
                                          document.getElementById('dualrefl_image').style.opacity = "1";
                                        }
                        
                                        function dualrefl_stop() {
                                          document.getElementById('dualrefl_image').style.opacity = "0";
                                        }
                                        dualrefl_stop()
                                    </script>
                                </td>
                                <td style="padding:20px;width:75%;vertical-align:middle">
                                        <papertitle>Consistency Policy: Accelerated Visuomotor Policies via Consistency Distillation</papertitle>
                                        <br>
                                        <a href="https://www.linkedin.com/in/aaditya-prasad/">Aaditya Prasad</a>,
                                        <strong>Kevin Lin</strong>,
                                        <a href="https://alexzhou907.github.io/">Linqi Zhou</a>,
                                        <a href="https://web.stanford.edu/~bohg/">Jeannette Bohg</a>
                                        <br>
                                    <p><a href="https://roboticsconference.org/">Robotics Science and Systems, 2024</a><br></p>
                                <p>
                                    We propose Consistency Policy, a faster and similarly powerful alternative to Diffusion Policy for learning visuomotor robotic manipulation policies. 
                                    We compare Consistency Policy with Diffusion Policy and other related speed-up methods across 6 simulation tasks as well as one real-world task where we demonstrate inference on a laptop GPU. 
                                    For all these tasks, Consistency Policy speeds up inference by an order of magnitude compared to the fastest alternative method and maintains competitive success rates. 
                                    </p>
                                </td>
                            </tr>

                            <tr onmouseout="dualrefl_stop()" onmouseover="dualrefl_start()">
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <div class="one">
                                        <div class="two" id='dualrefl_image'>
                                            <img src='images/droid.gif' width="160"></div>
                                        <img src='images/droid.gif' width="160">
                                    </div>
                                    <script type="text/javascript">
                                        function dualrefl_start() {
                                          document.getElementById('dualrefl_image').style.opacity = "1";
                                        }
                        
                                        function dualrefl_stop() {
                                          document.getElementById('dualrefl_image').style.opacity = "0";
                                        }
                                        dualrefl_stop()
                                    </script>
                                </td>
                                <td style="padding:20px;width:75%;vertical-align:middle">
                                    <a href="https://droid-dataset.github.io/">
                                        <papertitle>DROID: A Large-Scale In-The-Wild Robot Manipulation Dataset</papertitle>
                                    </a>
                                    <br>
                                    <a href="https://www.linkedin.com/in/alexander-khazatsky-b98841149/">Alexander Khazatsky*</a>,
                                    <a href="https://kpertsch.github.io/">Karl Pertsch*</a>,...,
                                    <strong>Kevin Lin</strong>, ...,
                                    <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>,
                                    <a href="https://ai.stanford.edu/~cbfinn/">Chelsea Finn</a>
                                    <br>
                                    <p><a href="https://roboticsconference.org/">Robotics Science and Systems, 2024</a><br></p>
                                <p>
                                    We introduce DROID, the most diverse robot manipulation dataset to date. It contains 76k demonstration trajectories or 350 hours of interaction data, 
                                    collected across 564 scenes and 84 tasks by 50 data collectors in North America, Asia, and Europe over the course of 12 months. 
                                    We demonstrate that training with DROID leads to policies with higher performance and improved generalization ability. 
                                    We open source the full dataset, policy learning code, and a detailed guide for reproducing our robot hardware setup.
                                </p>
                                </td>
                            </tr>

                            <tr onmouseout="dualrefl_stop()" onmouseover="dualrefl_start()">
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <div class="one">
                                        <div class="two" id="dualrefl_image">
                                          <video src="videos/t2m.mp4" width="160" autoplay loop muted></video>
                                        </div>
                                        <video src="videos/t2m.mp4" width="160" autoplay loop muted></video>
                                    </div>
                                    <script type="text/javascript">
                                        function dualrefl_start() {
                                          document.getElementById('dualrefl_image').style.opacity = "1";
                                        }
                        
                                        function dualrefl_stop() {
                                          document.getElementById('dualrefl_image').style.opacity = "0";
                                        }
                                        dualrefl_stop()
                                    </script>
                                </td>
                                <td style="padding:20px;width:75%;vertical-align:middle">
                                    <a href="https://sites.google.com/stanford.edu/text2motion">
                                        <papertitle>Text2Motion: From Natural Language Instructions to Feasible Plans</papertitle>
                                    </a>
                                    <br>
                                    <strong>Kevin Lin*</strong>,
                                    <a href="https://www.chrisagia.com">Christopher Agia*</a>,
                                    <a href="https://cs.stanford.edu/~takatoki/">Toki Migimatsu</a>,
                                    <a href="https://web.stanford.edu/~pavone/">Marco Pavone</a>,
                                    <a href="https://web.stanford.edu/~bohg/ ">Jeannette Bohg</a>
                                    <br>
                                    <p><a href="https://link.springer.com/article/10.1007/s10514-023-10131-7">Autonomous Robots, 2023 (Special Issue: Large Language Models in Robotics)</a><br><a href="https://microsoft.github.io/robotics.pretraining.workshop.icra/">ICRA 2023 Workshop on Pretraining for Robotics</a></p>
                                <p>
                                    We propose Text2Motion, a language-based planning framework enabling robots to solve sequential manipulation tasks that require long-horizon reasoning. 
                                    Given a natural language instruction, our framework constructs both a task- and motion-level plan that is verified to reach inferred symbolic goals. 
                                </p>
                                </td>
                            </tr>

                            <tr onmouseout="dualrefl_stop()" onmouseover="dualrefl_start()">
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <div class="one">
                                        <div class="two" id='dualrefl_image'>
                                            <img src='images/rtx_thumbnail.gif' width="160"></div>
                                        <img src='images/rtx_thumbnail.gif' width="160">
                                    </div>
                                    <script type="text/javascript">
                                        function dualrefl_start() {
                                          document.getElementById('dualrefl_image').style.opacity = "1";
                                        }
                        
                                        function dualrefl_stop() {
                                          document.getElementById('dualrefl_image').style.opacity = "0";
                                        }
                                        dualrefl_stop()
                                    </script>
                                </td>
                                <td style="padding:20px;width:75%;vertical-align:middle">
                                    <a href="https://robotics-transformer-x.github.io/">
                                        <papertitle>Open X-Embodiment: Robotic Learning Datasets and RT-X Models</papertitle>
                                    </a>
                                    <br>
                                    Open X-Embodiment Collaboration
                                    <!-- <br> -->
                                    <p><a href="https://2024.ieee-icra.org">IEEE International Conference on Robotics and Automation (ICRA), 2024</a> (<span style="color: #ff0000">Best Paper Award Finalist</span>)</p>
                                <p>
                                    We introduce the Open X-Embodiment Dataset, the largest robot learning dataset to date with 1M+ real robot trajectories, spanning 22 robot embodiments. 
                                    We train large, transformer-based policies on the dataset (RT-1-X, RT-2-X) and show that co-training with our diverse dataset substantially improves performance.
                                </p>
                                </td>
                            </tr>

                            <tr onmouseout="dualrefl_stop()" onmouseover="dualrefl_start()">
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <div class="one">
                                        <div class="two" id='dualrefl_image'>
                                            <img src='images/textured_sofa.png' width="160"></div>
                                        <img src='images/textured_sofa.png' width="160">
                                    </div>
                                    <script type="text/javascript">
                                        function dualrefl_start() {
                                          document.getElementById('dualrefl_image').style.opacity = "1";
                                        }
                        
                                        function dualrefl_stop() {
                                          document.getElementById('dualrefl_image').style.opacity = "0";
                                        }
                                        dualrefl_stop()
                                    </script>
                                </td>
                                <td style="padding:20px;width:75%;vertical-align:middle">
                                    <a href="https://imrss2022.github.io/contributions/lin.pdf">
                                        <papertitle>Partial-View Object View Synthesis via Filtering Inversion</papertitle>
                                    </a>
                                    <br>
                                    <a href="https://cs.stanford.edu/~sunfanyun">Fan-Yun Sun</a>,
                                    <a href="https://research.nvidia.com/person/jonathan-tremblay">Jonathan Tremblay</a>,
                                    <a href="https://www.cs.cornell.edu/~valts/">Valts Blukis</a>,
                                    <strong>Kevin Lin</strong>,
                                    <a href="https://faculty.cc.gatech.edu/~danfei/">Danfei Xu</a>,
                                    <a href="https://www.borisivanovic.com/">Boris Ivanovic</a>,
                                    <a href="http://karkus.tilda.ws/">Peter Karkus</a>,
                                    <a href="https://cecas.clemson.edu/~stb/">Stan Birchfield</a>,
                                    <a href="https://homes.cs.washington.edu/~fox/">Dieter Fox</a>,
                                    <a href="https://ai.stanford.edu/~zharu/">Ruohan Zhang</a>,
                                    <a href="https://yunzhuli.github.io/">Yunzhu Li</a>,
                                    <a href="https://jiajunwu.com/">Jiajun Wu</a>,
                                    <a href="https://research.nvidia.com/person/marco-pavone">Marco Pavone</a>,
                                    <a href="https://ed.stanford.edu/faculty/nhaber">Nick Haber</a>
                                    <br>
                                    <p><a href="https://3dvconf.github.io/2024/">International Conference on 3D Vision, 2024 (<span style="color: #ff0000">Oral Presentation</span>)</a></p>
                                <p>
                                    We propose a framework that combines the strengths of generative modeling and network finetun-ing to generate photorealistic 3D renderings of real-world objects
                                    from sparse and sequential RGB inputs.
                                </p>
                                </td>
                            </tr>

                            <tr onmouseout="dualrefl_stop()" onmouseover="dualrefl_start()">
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <div class="one">
                                        <div class="two" id='dualrefl_image'>
                                            <img src='images/rfe_camera_on_camera.jpg' width="160"></div>
                                        <img src='images/rfe_camera_on_camera.jpg' width="160">
                                    </div>
                                    <script type="text/javascript">
                                        function dualrefl_start() {
                                          document.getElementById('dualrefl_image').style.opacity = "1";
                                        }
                        
                                        function dualrefl_stop() {
                                          document.getElementById('dualrefl_image').style.opacity = "0";
                                        }
                                        dualrefl_stop()
                                    </script>
                                </td>
                                <td style="padding:20px;width:75%;vertical-align:middle">
                                    <a href="https://imrss2022.github.io/contributions/lin.pdf">
                                        <papertitle>Active View Planning for Radiance Fields</papertitle>
                                    </a>
                                    <br>
                                    <strong>Kevin Lin</strong>,
                                    <a href="https://brentyi.com/ ">Brent Yi</a>
                                    <br>
                                    <p><a href="https://imrss2022.github.io/">Robotics Science and Systems: Workshop on Implicit Representations for Robot Manipulation, 2022</a> (<span style="color: #ff0000">Spotlight Presentation</span>)</p>
                                <p>
                                    We motivate, discuss, and present a study on the problem of view planning for radiance fields. We introduce a benchmark, <a href="https://github.com/kevin-thankyou-lin/active-3d-gym">active-3d-gym</a>,
                                    for evaluating view planning algorithms for radiance field reconstructions and propose a simple solution to the view planning problem based on <i>radiance field ensembles</i>.
                                </p>
                                </td>
                            </tr> 
                            
                            <tr onmouseout="dualrefl_stop()" onmouseover="dualrefl_start()">
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <div class="one">
                                        <div class="two" id='dualrefl_image'>
                                            <img src='images/LBWayPt-Nav-3d.jpg' width="160"></div>
                                        <img src='images/LBWayPt-Nav-3d.jpg' width="160">
                                    </div>
                                    <script type="text/javascript">
                                        function dualrefl_start() {
                                          document.getElementById('dualrefl_image').style.opacity = "1";
                                        }
                        
                                        function dualrefl_stop() {
                                          document.getElementById('dualrefl_image').style.opacity = "0";
                                        }
                                        dualrefl_stop()
                                    </script>
                                </td>
                                <td style="padding:20px;width:75%;vertical-align:middle">
                                    <a href="https://arxiv.org/abs/2112.03554">
                                        <papertitle>Combining optimal control and learning for autonomous aerial navigation in novel indoor environments</papertitle>
                                    </a>
                                    <br>
                                    <strong>Kevin Lin</strong>,
                                    <a>Brian Huo</a>,
                                    <a>Megan Hu</a>
                                    <br>
                                    <p><a href="https://arxiv.org/abs/2112.03554">Arxiv, 2021</a></p>
                                <p>
                                    We study how aerial robots can autonomously learn to navigate safely in novel indoor environments by combining optimal control and learning techniques. We train our agent entirely in simulation and demonstrate generalization on novel indoor scenes.
                                </p>
                                </td>
                            </tr> 

                            <tr onmouseout="dualrefl_stop() " onmouseover="dualrefl_start() ">
                                <td style="padding:20px;width:25%;vertical-align:middle ">
                                    <div class="one ">
                                        <div class="two " id='dualrefl_image'>
                                            <img src='images/beliefandlevelkreasoningimg.png' width="160 "></div>
                                        <img src='images/beliefandlevelkreasoningimg.png' width="160 ">
                                    </div>
                                    <script type="text/javascript ">
                                        function dualrefl_start() {
                                        document.getElementById('dualrefl_image').style.opacity = "1 ";
                                        }
                        
                                        function dualrefl_stop() {
                                        document.getElementById('dualrefl_image').style.opacity = "0 ";
                                        }
                                        dualrefl_stop()
                                    </script>
                                </td>
                                <td style="padding:20px;width:75%;vertical-align:middle ">
                                    <a href="https://drive.google.com/file/d/1fQJoQsX_L4x-bw-tMBO28KrypIcxSrqT/view ">
                                        <papertitle>Beliefs and Level-k Reasoning in Traffic</papertitle>
                                    </a>
                                    <br>
                                    <a href="https://eugenevinitsky.github.io/ ">E Vinitsky</a>,
                                    <a href="https://filangel.github.io/website/ ">A Filos</a>,
                                    <strong>Kevin Lin</strong>,
                                    <a href="https://www.linkedin.com/in/nickl77/ ">N Liu</a>,
                                    <a href="https://github.com/nathanlct ">N Lichtle</a>,
                                    <a href="https://people.eecs.berkeley.edu/~anca/ ">A Dragan</a>,
                                    <a href="https://bayen.berkeley.edu/alex-bayen ">A Bayen</a>,
                                    <a href="https://people.eecs.berkeley.edu/~rmcallister/ ">R McAllister</a>,
                                    <a href="https://www.jakobfoerster.com/">J Foerster</a>

                                    <br>
                                    <br>
                                    <p><a href="https://sites.google.com/view/emecom2020/home?authuser=0 ">NeurIPS: Workshop on Emergent Communication, 2020</a></p>
                                    <p></p>
                                    <p>
                                        Occlusions present a major obstacle to guaranteeing safety in autonomous driving. Our key insight is that, sometimes, 
                                        AV can get information about occluded regions by inferring over the actions of other agents on the road. 
                                        We demonstrate that AVs can use this inferred data and level-K reasoning to avoid collisions with occluded pedestrians and drive in a pro-social manner.
                                    </p>
                                </td>
                            </tr>

                        </tbody>
                    </table>

                    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <heading>Projects</heading>
                                </td>
                            </tr>
                        </tbody>
                    </table>

                    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>

                            <tr onmouseout="dualrefl_stop()" onmouseover="dualrefl_start()">
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <div class="one">
                                        <div class="two" id='dualrefl_image'>
                                            <img src='images/tesroo.jpg' width="160"></div>
                                        <img src='images/tesroo.jpg' width="160">
                                    </div>
                                    <script type="text/javascript">
                                        function dualrefl_start() {
                                          document.getElementById('dualrefl_image').style.opacity = "1";
                                        }
                        
                                        function dualrefl_stop() {
                                          document.getElementById('dualrefl_image').style.opacity = "0";
                                        }
                                        dualrefl_stop()
                                    </script>
                                </td>
                                <td style="padding:20px;width:75%;vertical-align:middle">
                                    
                                    <papertitle><a href= "https://sites.google.com/berkeley.edu/tesroo-106a/home?authuser=0">Tesroo: A Redesigned Vacuum Robot</a></papertitle>

                                    <br>
                                    <br>
                                    <p>Won 1st place in UC Berkeley robotics course's final project</a></p>

                      <p>
                        Envisioned and built a prototype vision-only robot vacuum using Visual SLAM to compete with LiDAR based Roomba models.
                      </p>
                    </td>
                  </tr> 

                  <tr onmouseout="dualrefl_stop()" onmouseover="dualrefl_start()">
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <div class="two" id='dualrefl_image'>
                                <img src='images/flow.jpg' width="160"></div>
                            <img src='images/flow.jpg' width="160">
                        </div>
                        <script type="text/javascript">
                            function dualrefl_start() {
                              document.getElementById('dualrefl_image').style.opacity = "1";
                            }
            
                            function dualrefl_stop() {
                              document.getElementById('dualrefl_image').style.opacity = "0";
                            }
                            dualrefl_stop()
                        </script>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        
                        <papertitle><a href= "https://flow-project.github.io/">FLOW: A deep reinforcement learning framework for mixed autonomy traffic</a></papertitle>

                        <br>
                        <br>
                        <p>
                            Developed an open-source tool for applying ML techniques to autonomous vehicle driving policy discovery
                        </p>
                        </td>
                    </tr> 

                        </tbody>
                    </table>


                    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                        <tbody>
                            <tr>
                                <td>
                                    <heading>Teaching</heading>
                                    <h4>Feel free to share your feedback <a href="https://docs.google.com/forms/d/e/1FAIpQLSfSj89DF2Yxgc_Er93d0D91z-bTjREtTA9bGsDelAfYRdIkkA/viewform">here</a></h4>
                                </td>
                            </tr>
                        </tbody>
                    </table>
                    <table width="100%" align="center" border="0" cellpadding="20">
                        <tbody>
                            <!-- Stanford Course -->
                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img width="75%" src="images/stanford-logo.png" alt="Stanford Logo">
                                </td>
                                <td width="100%" valign="center">
                                    <a href="https://cs221.stanford.edu/">Teaching Assistant, CS 221 Fall 2023, Fall 2024</a>
                                    <p>Introduction to Artificial Intelligence</p>
                                    <p><strong>Instructors:</strong> Percy Liang, Dorsa Sadigh</p>
                                    <br>
                                    <br>
                                </td>
                            </tr>
                            <!-- Berkeley Courses -->
                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img width="75%" src="images/170pnpenguin.png" alt="cs170">
                                </td>
                                <td width="75%" valign="center">
                                    <a href="https://inst.eecs.berkeley.edu/~ee126/sp21/">Undergraduate Student Instructor, EE 126 Spring 2021</a>
                                    <p>Probability and Random Processes</p>
                                    <a href="https://cs170.org/">Undergraduate Student Instructor, CS 170 Fall 2020</a>
                                    <p>Efficient Algorithms and Intractable Problems</p>
                                    <a href="http://www.eecs70.org/">Undergraduate Student Instructor, CS 70 Summer 2020</a>
                                    <p>Discrete Math and Probability Theory</p>
                                    <br>
                                    <br>
                                </td>
                            </tr>
                        </tbody>
                    </table>
</body>

</html>
